{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from ListOpsDataset import ListOpsDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "\n",
      "Dataset sizes:\n",
      "Training: 96000 examples\n",
      "Validation: 2000 examples\n",
      "Test: 2000 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the data directory and file paths\n",
    "data_dir = '../data/output_dir/'\n",
    "train_file = os.path.join(data_dir, 'basic_train.tsv')\n",
    "val_file = os.path.join(data_dir, 'basic_val.tsv')\n",
    "test_file = os.path.join(data_dir, 'basic_test.tsv')\n",
    "\n",
    "def load_listops_data(file_path, max_rows=None):\n",
    "    \"\"\"\n",
    "    Load ListOps data from TSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the TSV file\n",
    "        max_rows: Maximum number of rows to load (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        sources: Array of source expressions\n",
    "        targets: Array of target values (0-9)\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    targets = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # Skip header (Source, Target)\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows and i >= max_rows:\n",
    "                break\n",
    "            if not line.strip():  # Skip empty lines\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip lines that don't have exactly two columns\n",
    "            source, target = parts\n",
    "            sources.append(source)\n",
    "            targets.append(int(target))  # Target is always 0-9\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    source_array = np.array(sources, dtype=object)  # Keep expressions as strings\n",
    "    target_array = np.array(targets, dtype=np.int32)  # Targets are integers\n",
    "    \n",
    "    return source_array, target_array\n",
    "\n",
    "try:\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    X_train, y_train = load_listops_data(train_file)\n",
    "    \n",
    "    # Load validation data\n",
    "    print(\"Loading validation data...\")\n",
    "    X_val, y_val = load_listops_data(val_file)\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    X_test, y_test = load_listops_data(test_file)\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(\"\\nDataset sizes:\")\n",
    "    print(f\"Training: {len(X_train)} examples\")\n",
    "    print(f\"Validation: {len(X_val)} examples\")\n",
    "    print(f\"Test: {len(X_test)} examples\")\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 96000\n",
      "Val: 2000\n",
      "Test: 2000\n",
      "\n",
      "First batch shape:\n",
      "Input IDs: torch.Size([32, 2000])\n",
      "Targets: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = ListOpsDataset(X_train, y_train)\n",
    "val_dataset = ListOpsDataset(X_val, y_val)\n",
    "test_dataset = ListOpsDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Verify the data\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Val: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")\n",
    "\n",
    "# Check first batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nFirst batch shape:\")\n",
    "print(f\"Input IDs: {batch['input_ids'].shape}\")\n",
    "print(f\"Targets: {batch['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total parameters: 7498\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3000/3000 [05:22<00:00,  9.30it/s, loss=0.0705, acc=17.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Train Loss: 2.2545 | Train Acc: 17.01%\n",
      "Val Loss: 2.2598 | Val Acc: 17.15%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3000/3000 [05:20<00:00,  9.35it/s, loss=0.0704, acc=16.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5\n",
      "Train Loss: 2.2515 | Train Acc: 16.82%\n",
      "Val Loss: 2.2575 | Val Acc: 17.15%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3000/3000 [05:20<00:00,  9.37it/s, loss=0.0704, acc=16.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5\n",
      "Train Loss: 2.2536 | Train Acc: 16.84%\n",
      "Val Loss: 2.2672 | Val Acc: 15.95%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3000/3000 [05:28<00:00,  9.12it/s, loss=0.0705, acc=16.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5\n",
      "Train Loss: 2.2559 | Train Acc: 16.84%\n",
      "Val Loss: 2.2778 | Val Acc: 15.95%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3000/3000 [05:35<00:00,  8.93it/s, loss=0.0705, acc=16.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5\n",
      "Train Loss: 2.2553 | Train Acc: 16.75%\n",
      "Val Loss: 2.2637 | Val Acc: 17.15%\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Loss: 2.2508 | Test Acc: 17.80%\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size=18,      # Size of vocabulary\n",
    "                 embedding_dim=32,    # Small embedding dimension\n",
    "                 hidden_dim=64,      # Hidden state dimension\n",
    "                 num_layers=1,       # Single RNN layer\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 10)  #   (0-9)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        x = self.embedding(x)        # (batch_size, sequence_length, embedding_dim)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        output, hidden = self.rnn(x)  # output: (batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        x = hidden[-1]              # (batch_size, hidden_dim)\n",
    "        x = self.fc(x)              # (batch_size, 10)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleRNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#focal loss\n",
    "criterion = nn.FocalLoss(gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{train_loss/total:.4f}', \n",
    "                         'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {100.*correct/total:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "   \n",
    "    print('-' * 50)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f'\\nTest Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, add this class definition for Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Then modify the criterion initialization\n",
    "criterion = FocalLoss(gamma=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total parameters: 7498\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 3000/3000 [05:36<00:00,  8.90it/s, loss=0.0565, acc=16.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "Train Loss: 1.8067 | Train Acc: 16.75%\n",
      "Val Loss: 1.8135 | Val Acc: 16.00%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 3000/3000 [05:38<00:00,  8.86it/s, loss=0.0565, acc=16.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3\n",
      "Train Loss: 1.8065 | Train Acc: 16.80%\n",
      "Val Loss: 1.8265 | Val Acc: 15.95%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 3000/3000 [05:37<00:00,  8.89it/s, loss=0.0565, acc=16.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3\n",
      "Train Loss: 1.8085 | Train Acc: 16.96%\n",
      "Val Loss: 1.8115 | Val Acc: 17.15%\n",
      "--------------------------------------------------\n",
      "\n",
      "Test Loss: 1.7908 | Test Acc: 17.80%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleRNNModel().to(device)\n",
    "#focal loss\n",
    "criterion = FocalLoss(gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{train_loss/total:.4f}', \n",
    "                         'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {100.*correct/total:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "   \n",
    "    print('-' * 50)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f'\\nTest Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lra_env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
