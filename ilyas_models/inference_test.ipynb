{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-0.0331,  0.0453, -0.1603, -0.1331, -0.0699, -0.1504,  0.0052, -0.0163,\n",
      "         -0.3249, -0.0800]])\n",
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Specify the path to the trained model checkpoints\n",
    "model_path = \"checkpoint-4690\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Text to classify\n",
    "text = \"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the logits and predicted class\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ( ( ( ( ( ( ( [MED ( ( ( ( ( ( [MED 2 ) ( ( ( ( ( ( [SM 4 ) 1 ) 9 ) 2 ) 9 ) ] ) ) ( ( ( ( [MAX ( ( ( ( ( ( ( [MAX 9 ) 4 ) 1 ) 6 ) 7 ) 4 ) ] ) ) ( ( ( ( ( ( ( ( ( ( ( [MIN 9 ) ( ( ( ( ( ( ( ( [MIN 4 ) ( ( ( ( ( ( ( ( ( ( ( [MED 0 ) ( ( ( ( ( [MED ( ( ( [MED 2 ) ( ( ( ( ( ( ( [MIN 3 ) 6 ) 8 ) 1 ) 6 ) 9 ) ] ) ) ] ) ) 4 ) ( ( ( ( ( ( ( ( ( [MIN 3 ) 2 ) 4 ) ( ( ( ( ( ( ( ( ( [SM 3 ) 4 ) 4 ) 3 ) 9 ) 2 ) 0 ) 7 ) ] ) ) 4 ) 9 ) 3 ) ( ( ( ( ( ( [MED 4 ) 6 ) 5 ) 1 ) 8 ) ] ) ) ] ) ) 9 ) ] ) ) 4 ) 3 ) ( ( ( ( ( ( ( ( ( [MIN 1 ) ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( ( ( ( [MED 8 ) 3 ) 4 ) 4 ) 3 ) 9 ) 0 ) 4 ) 7 ) 9 ) ] ) ) 8 ) 3 ) 3 ) ] ) ) 4 ) 2 ) 7 ) 4 ) 2 ) ( ( ( ( ( ( ( ( [MED ( ( ( [SM 3 ) 8 ) ] ) ) 4 ) 6 ) 8 ) 9 ) ( ( ( ( ( [MED 5 ) 1 ) 2 ) 5 ) ] ) ) 5 ) ] ) ) ] ) ) 3 ) 3 ) 0 ) ( ( ( ( ( ( ( ( [SM 9 ) ( ( ( ( ( ( [SM 1 ) ( ( ( ( ( ( ( [SM 0 ) 8 ) 9 ) 7 ) 6 ) 8 ) ] ) ) 9 ) ( ( ( ( ( ( ( [MIN 1 ) 6 ) 3 ) 0 ) 8 ) 4 ) ] ) ) 0 ) ] ) ) 5 ) 1 ) 5 ) 1 ) 6 ) ] ) ) ( ( ( ( ( ( [MED 8 ) 8 ) ( ( ( ( ( ( [MIN 7 ) ( ( ( ( ( ( [MAX 3 ) 6 ) 3 ) 7 ) 7 ) ] ) ) ( ( ( ( ( ( ( ( ( ( [MAX 9 ) 2 ) 5 ) 7 ) 2 ) 5 ) 0 ) 0 ) 1 ) ] ) ) 3 ) ( ( ( [MED 6 ) 0 ) ] ) ) ] ) ) 5 ) ( ( ( ( ( ( ( ( [SM ( ( ( ( ( ( ( [MIN 9 ) 9 ) 5 ) 8 ) 9 ) 9 ) ] ) ) ( ( ( ( [MIN 4 ) 0 ) 0 ) ] ) ) 1 ) 7 ) 0 ) 9 ) ( ( ( ( ( ( ( ( ( ( ( [MED 9 ) 9 ) 3 ) 0 ) 0 ) 9 ) 5 ) 0 ) 9 ) 6 ) ] ) ) ] ) ) ] ) ) ] ) ) 4 ) 4 ) 4 ) 2 ) 3 ) ] ) ) ( ( ( ( ( ( ( [SM 6 ) 8 ) ( ( ( ( ( ( [MIN 6 ) 3 ) 4 ) 0 ) 9 ) ] ) ) ( ( ( ( ( ( ( ( ( [MED 9 ) 5 ) 1 ) 9 ) 0 ) 1 ) ( ( ( ( ( ( ( ( ( [SM 8 ) 8 ) 1 ) 6 ) ( ( ( ( ( ( ( ( ( ( [MED 9 ) 3 ) 2 ) 0 ) 0 ) 1 ) 5 ) 8 ) 6 ) ] ) ) ( ( ( ( ( ( ( ( ( ( [MIN ( ( ( ( ( ( ( ( ( ( [SM 3 ) 9 ) 5 ) 9 ) 8 ) 1 ) 6 ) 8 ) 0 ) ] ) ) ( ( ( ( ( [MED 3 ) 6 ) 5 ) 6 ) ] ) ) 6 ) 4 ) 6 ) 0 ) ( ( ( ( ( ( ( ( [SM 1 ) 6 ) 6 ) 8 ) 2 ) 7 ) 8 ) ] ) ) 7 ) 6 ) ] ) ) ( ( ( ( ( ( ( ( [MAX 5 ) 6 ) 8 ) ( ( ( ( ( ( ( ( ( ( [SM 9 ) 8 ) 8 ) 7 ) 4 ) 5 ) 6 ) 9 ) 6 ) ] ) ) 1 ) 4 ) ( ( ( ( ( ( ( ( [SM 3 ) 8 ) 9 ) 4 ) 8 ) 2 ) 6 ) ] ) ) ] ) ) 8 ) ] ) ) 7 ) ] ) ) 7 ) ( ( ( ( ( ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( [MED 1 ) 1 ) 2 ) 2 ) 4 ) 2 ) ( ( ( ( ( ( ( ( ( ( ( [MIN ( ( ( ( ( ( ( ( [MIN 5 ) 8 ) 2 ) 7 ) 6 ) 8 ) 3 ) ] ) ) 9 ) ( ( ( ( ( ( [MIN 1 ) 9 ) 8 ) 4 ) 8 ) ] ) ) 0 ) 9 ) ( ( ( ( ( ( [MIN 4 ) 1 ) 3 ) 8 ) 4 ) ] ) ) 1 ) 3 ) 3 ) ( ( ( ( ( ( ( [MIN 0 ) 8 ) 0 ) 8 ) 0 ) 2 ) ] ) ) ] ) ) ] ) ) ( ( ( ( ( ( ( ( ( ( [MED ( ( ( ( [SM 5 ) 8 ) 9 ) ] ) ) 1 ) 3 ) 8 ) 5 ) 7 ) 9 ) ( ( ( ( ( ( ( [MAX 6 ) 0 ) ( ( ( ( [MED 0 ) 1 ) 3 ) ] ) ) 6 ) 8 ) 1 ) ] ) ) 2 ) ] ) ) ( ( ( ( ( [SM 3 ) 6 ) 6 ) ( ( ( ( ( ( ( ( [MIN 7 ) 6 ) 3 ) 0 ) 7 ) 6 ) 4 ) ] ) ) ] ) ) 2 ) ( ( ( ( ( [MIN 2 ) 3 ) 9 ) 4 ) ] ) ) 6 ) ( ( ( ( [MIN 4 ) 4 ) 7 ) ] ) ) 8 ) 2 ) ] ) ) ] ) ) 7 ) ( ( ( ( ( ( ( ( ( ( ( [SM 5 ) 9 ) 4 ) 3 ) 8 ) 2 ) ( ( ( ( ( ( ( ( [SM 6 ) 6 ) 6 ) ( ( ( ( ( ( ( ( ( ( ( [SM 1 ) 7 ) ( ( ( ( ( ( [MED 9 ) 1 ) ( ( ( ( ( ( ( ( [MED 9 ) 1 ) 7 ) 4 ) 7 ) 5 ) 8 ) ] ) ) ( ( ( [SM 1 ) 9 ) ] ) ) 8 ) ] ) ) 8 ) 5 ) 6 ) 2 ) 4 ) 7 ) 3 ) ] ) ) 1 ) 7 ) 8 ) ] ) ) ( ( ( ( ( ( ( [MAX 4 ) 5 ) 2 ) 9 ) 0 ) 3 ) ] ) ) 1 ) 8 ) ] ) ) ( ( ( ( ( ( ( ( [SM 8 ) ( ( ( ( ( ( ( ( [MIN 3 ) 1 ) 0 ) 3 ) ( ( ( ( [MAX 0 ) ( ( ( ( ( ( [MAX 7 ) 6 ) ( ( ( [MAX 3 ) 2 ) ] ) ) 2 ) 2 ) ] ) ) 9 ) ] ) ) 3 ) 4 ) ] ) ) ( ( ( [MIN 8 ) 5 ) ] ) ) 6 ) 0 ) ( ( ( ( ( ( [MAX 0 ) ( ( ( [MIN 8 ) 0 ) ] ) ) 9 ) 2 ) ( ( ( [MED 4 ) 0 ) ] ) ) ] ) ) ( ( ( ( ( ( ( ( ( [MED 3 ) ( ( ( [MED 9 ) 6 ) ] ) ) ( ( ( ( ( [MAX ( ( ( ( ( [MAX 3 ) 9 ) ( ( ( ( ( ( [SM 5 ) 0 ) 0 ) 6 ) 6 ) ] ) ) ( ( ( ( ( ( [SM 2 ) 1 ) 7 ) 1 ) 0 ) ] ) ) ] ) ) ( ( ( [SM 7 ) 9 ) ] ) ) 4 ) 8 ) ] ) ) 2 ) 0 ) 7 ) 4 ) 8 ) ] ) ) ] ) ) ( ( ( [MAX 8 ) 0 ) ] ) ) 5 ) 9 ) 5 ) ] ) ) 9 ) ] ) ) 7 ) ( ( ( ( ( ( ( ( ( ( [MIN ( ( ( ( ( ( ( ( [MIN 5 ) 6 ) 2 ) 8 ) ( ( ( ( ( ( ( ( ( [MIN ( ( ( ( ( ( ( [SM 8 ) ( ( ( ( ( ( ( ( ( ( [MED 9 ) 7 ) 8 ) 3 ) 2 ) 9 ) 3 ) 7 ) ( ( ( ( ( ( ( ( ( ( [MAX 9 ) 1 ) 5 ) 3 ) 7 ) 0 ) 7 ) 3 ) 1 ) ] ) ) ] ) ) 7 ) ( ( ( ( ( ( ( ( ( ( ( [MAX 1 ) 6 ) 6 ) 4 ) 6 ) ( ( ( ( ( ( ( [MAX 3 ) ( ( ( ( ( [SM 4 ) 2 ) 8 ) 9 ) ] ) ) 8 ) 2 ) 0 ) ( ( ( ( ( ( ( ( ( [SM 4 ) 0 ) 2 ) 6 ) 1 ) 3 ) 2 ) 1 ) ] ) ) ] ) ) 0 ) 4 ) ( ( ( ( [SM 2 ) ( ( ( ( ( ( ( ( [MED 5 ) 9 ) 0 ) 7 ) 1 ) 4 ) 7 ) ] ) ) 7 ) ] ) ) 5 ) ] ) ) 1 ) 0 ) ] ) ) 0 ) 6 ) 5 ) 3 ) 8 ) 7 ) 1 ) ] ) ) 9 ) 6 ) ] ) ) 7 ) 7 ) 8 ) ( ( ( ( ( ( ( ( ( ( [SM 3 ) 5 ) ( ( ( ( ( ( ( [SM 7 ) 4 ) 8 ) 6 ) ( ( ( ( ( ( ( ( ( [SM ( ( ( ( ( ( ( ( ( ( ( [SM 2 ) 5 ) ( ( ( ( ( [MED 0 ) 9 ) ( ( ( [MAX 7 ) 5 ) ] ) ) 7 ) ] ) ) 0 ) 4 ) 2 ) 3 ) ( ( ( ( ( ( ( [MIN 2 ) 3 ) 8 ) 2 ) 5 ) ( ( ( ( ( ( ( ( [MIN 1 ) 4 ) 3 ) 7 ) 9 ) 3 ) 2 ) ] ) ) ] ) ) 8 ) 8 ) ] ) ) 4 ) 9 ) 5 ) 0 ) 4 ) ( ( ( ( ( ( ( ( ( ( [MIN 9 ) ( ( ( ( ( ( ( [MED ( ( ( ( ( ( ( ( ( ( [SM 0 ) 4 ) 2 ) 3 ) 3 ) 6 ) 6 ) 0 ) 9 ) ] ) ) 1 ) 2 ) 4 ) 0 ) 2 ) ] ) ) 6 ) 7 ) 6 ) 5 ) 8 ) ( ( ( ( ( [SM 6 ) 7 ) ( ( ( ( ( ( ( ( ( ( ( [MIN 6 ) 6 ) 1 ) 5 ) 0 ) 7 ) 5 ) 5 ) 7 ) 4 ) ] ) ) 4 ) ] ) ) 0 ) ] ) ) ( ( ( ( [MAX ( ( ( ( [MIN 5 ) 0 ) 1 ) ] ) ) 8 ) 6 ) ] ) ) ] ) ) ( ( ( ( ( ( ( ( ( ( [SM 4 ) 7 ) 0 ) 3 ) 8 ) 6 ) 4 ) ( ( ( ( ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( ( [MED 6 ) 2 ) 6 ) 3 ) 6 ) ( ( ( [MIN 3 ) 7 ) ] ) ) 7 ) 8 ) ] ) ) 7 ) 2 ) 9 ) ( ( ( [MIN 4 ) 4 ) ] ) ) 9 ) ( ( ( ( ( ( ( ( ( ( [SM 7 ) 7 ) 7 ) 0 ) 8 ) 0 ) ( ( ( [SM 2 ) 9 ) ] ) ) 3 ) ( ( ( [MIN 2 ) 0 ) ] ) ) ] ) ) 4 ) ] ) ) 0 ) ] ) ) ] ) ) 4 ) 9 ) ( ( ( ( ( ( [MIN 7 ) 8 ) 0 ) 4 ) ( ( ( ( ( ( ( ( ( ( ( [SM ( ( ( ( ( ( ( [MED 8 ) 2 ) ( ( ( [MIN ( ( ( ( ( ( ( ( ( [SM 3 ) 7 ) 3 ) 0 ) 1 ) 5 ) 1 ) 8 ) ] ) ) 5 ) ] ) ) ( ( ( ( ( ( ( ( ( ( ( [MED 5 ) 8 ) ( ( ( ( ( ( ( ( ( ( [MAX 4 ) 5 ) 9 ) 2 ) 2 ) 2 ) 1 ) 5 ) 4 ) ] ) ) 2 ) 0 ) 3 ) 6 ) ( ( ( ( ( ( ( ( ( ( [MAX 6 ) 2 ) 6 ) 6 ) 8 ) 7 ) 6 ) 7 ) 6 ) ] ) ) 9 ) 1 ) ] ) ) ( ( ( ( ( [SM 6 ) 2 ) 0 ) 4 ) ] ) ) ( ( ( ( ( [MIN ( ( ( ( [SM 6 ) 0 ) 9 ) ] ) ) 6 ) 9 ) ( ( ( [MAX 1 ) 6 ) ] ) ) ] ) ) ] ) ) 6 ) ( ( ( ( ( ( ( ( ( [MIN 0 ) ( ( ( ( ( ( [MIN ( ( ( ( [MED 2 ) 3 ) 9 ) ] ) ) 3 ) 9 ) 9 ) 3 ) ] ) ) 3 ) 6 ) ( ( ( ( ( ( ( [MAX 4 ) 0 ) ( ( ( ( ( ( [MIN 3 ) 0 ) 4 ) 3 ) 7 ) ] ) ) 4 ) 5 ) 4 ) ] ) ) ( ( ( ( ( ( ( [SM 2 ) ( ( ( ( ( [MED 1 ) 5 ) 1 ) 1 ) ] ) ) 1 ) 7 ) 6 ) 6 ) ] ) ) ( ( ( ( ( ( ( ( ( [SM 4 ) 7 ) 8 ) 8 ) ( ( ( ( ( ( ( ( [SM 8 ) 4 ) 7 ) 8 ) 9 ) 3 ) 2 ) ] ) ) 4 ) 6 ) 5 ) ] ) ) 1 ) ] ) ) ( ( ( ( ( ( ( ( ( [SM 6 ) 4 ) 3 ) 0 ) 3 ) 9 ) 5 ) 4 ) ] ) ) 3 ) ( ( ( ( ( ( ( ( ( ( [MIN 1 ) ( ( ( ( ( ( ( ( ( ( [MED 2 ) 3 ) 1 ) 1 ) 9 ) 0 ) 3 ) 7 ) 9 ) ] ) ) 2 ) 6 ) 2 ) 0 ) 2 ) ( ( ( [MAX 9 ) ( ( ( ( [MAX 2 ) 7 ) 7 ) ] ) ) ] ) ) 1 ) ] ) ) 8 ) 7 ) ( ( ( ( ( ( ( ( ( [MED 9 ) 9 ) 7 ) 8 ) 4 ) 5 ) 5 ) ( ( ( ( ( ( ( [MED 8 ) 7 ) ( ( ( ( ( ( ( ( ( [MIN 5 ) 6 ) 9 ) 3 ) 0 ) 1 ) 2 ) 3 ) ] ) ) 5 ) 7 ) 2 ) ] ) ) ] ) ) 9 ) ] ) ) ] ) ) ( ( ( ( ( ( ( ( ( [MED 5 ) 8 ) 5 ) 3 ) 8 ) ( ( ( ( ( ( ( [MAX 4 ) ( ( ( ( ( ( ( ( ( ( ( [MAX 7 ) 4 ) 1 ) 6 ) ( ( ( ( ( ( ( ( ( [MAX 3 ) 8 ) 7 ) 5 ) 9 ) 5 ) 2 ) ( ( ( ( ( ( ( ( ( ( [MIN 8 ) 2 ) 7 ) 3 ) 9 ) 9 ) 8 ) 1 ) 5 ) ] ) ) ] ) ) ( ( ( ( ( [MIN 1 ) 7 ) ( ( ( ( ( ( ( ( [MAX 3 ) 9 ) 3 ) 6 ) 3 ) 8 ) 5 ) ] ) ) ( ( ( ( ( ( [MIN 7 ) 3 ) 6 ) 0 ) 9 ) ] ) ) ] ) ) 3 ) 2 ) 7 ) 5 ) ] ) ) 2 ) 7 ) 4 ) 2 ) ] ) ) 1 ) 6 ) ] ) ) 6 ) 5 ) ] ) ) 1 ) ( ( ( [MIN ( ( ( ( ( ( ( ( [MIN 8 ) 2 ) 0 ) ( ( ( ( ( ( ( [SM 5 ) 4 ) 8 ) 1 ) 9 ) 1 ) ] ) ) 9 ) 8 ) 1 ) ] ) ) ( ( ( ( ( ( ( [MIN 1 ) ( ( ( ( [MED 8 ) 7 ) 7 ) ] ) ) 4 ) ( ( ( ( ( ( ( [SM 0 ) 3 ) ( ( ( ( ( ( ( ( ( ( ( [MAX 2 ) 4 ) 3 ) 5 ) 9 ) ( ( ( ( ( ( ( ( ( ( [MED 6 ) ( ( ( ( ( ( ( ( ( ( ( [MAX 4 ) 9 ) 3 ) 6 ) 0 ) 2 ) 1 ) 6 ) 4 ) 8 ) ] ) ) 1 ) 3 ) ( ( ( ( ( ( ( ( ( ( [SM 0 ) 5 ) 6 ) 0 ) 4 ) 1 ) 6 ) 8 ) 0 ) ] ) ) 2 ) 2 ) 4 ) 5 ) ] ) ) 8 ) 1 ) 4 ) ( ( ( ( ( [MIN ( ( ( ( ( ( ( ( ( ( ( [SM 5 ) 8 ) 4 ) 2 ) 4 ) 9 ) 3 ) 9 ) 3 ) 0 ) ] ) ) 5 ) 5 ) 4 ) ] ) ) ] ) ) ( ( ( [MIN ( ( ( ( ( ( ( ( ( [MAX 2 ) 9 ) 3 ) 7 ) ( ( ( ( [MIN 3 ) 0 ) 8 ) ] ) ) 5 ) 8 ) 2 ) ] ) ) 9 ) ] ) ) 7 ) 7 ) ] ) ) ( ( ( ( ( ( ( ( ( ( [SM 4 ) ( ( ( ( ( ( ( ( ( ( [MED 1 ) 5 ) 2 ) 8 ) 0 ) ( ( ( ( ( ( ( ( [MIN 0 ) 1 ) ( ( ( ( ( ( ( [SM 2 ) 9 ) 4 ) 0 ) 2 ) 8 ) ] ) ) 9 ) ( ( ( ( ( ( ( [SM 7 ) 1 ) 0 ) 6 ) 4 ) 4 ) ] ) ) 9 ) ( ( ( ( ( ( ( ( ( [SM 8 ) 4 ) 5 ) 9 ) 0 ) 7 ) 2 ) 6 ) ] ) ) ] ) ) 3 ) ( ( ( ( ( [MAX ( ( ( ( ( ( ( ( ( [MAX 5 ) 0 ) 9 ) 7 ) 9 ) 9 ) 2 ) 5 ) ] ) ) 9 ) ( ( ( ( ( [MIN 8 ) 2 ) 7 ) 7 ) ] ) ) ( ( ( ( ( ( ( [MIN 5 ) 3 ) 7 ) 4 ) 6 ) 3 ) ] ) ) ] ) ) ( ( ( ( ( [MED 9 ) 1 ) ( ( ( ( ( ( [SM 9 ) 3 ) 2 ) 8 ) 1 ) ] ) ) 0 ) ] ) ) ] ) ) ( ( ( ( ( ( ( ( ( ( [MIN 4 ) ( ( ( ( ( ( ( [SM 6 ) 4 ) 6 ) ( ( ( ( ( [SM 8 ) 8 ) 3 ) 1 ) ] ) ) 0 ) 9 ) ] ) ) 9 ) 7 ) 1 ) ( ( ( ( ( ( ( ( ( [SM ( ( ( ( ( [MED 4 ) 5 ) 5 ) 0 ) ] ) ) ( ( ( ( ( ( ( [MAX 3 ) 2 ) 6 ) 2 ) 3 ) 1 ) ] ) ) ( ( ( ( ( ( ( ( ( [SM 0 ) 8 ) 1 ) 5 ) 3 ) 0 ) 6 ) 1 ) ] ) ) 6 ) ( ( ( ( ( ( ( ( [MAX 3 ) 4 ) 8 ) 5 ) 4 ) 0 ) 4 ) ] ) ) 1 ) 4 ) 0 ) ] ) ) 2 ) 7 ) 8 ) ] ) ) ( ( ( ( ( ( ( ( ( [SM ( ( ( ( ( ( ( ( ( ( ( [MIN ( ( ( ( ( ( ( [SM 8 ) 0 ) 3 ) 9 ) 0 ) 9 ) ] ) ) 8 ) 3 ) 1 ) 0 ) 6 ) ( ( ( [MAX 5 ) 8 ) ] ) ) 2 ) ( ( ( ( ( ( ( ( ( ( ( [MED 2 ) 4 ) 9 ) 0 ) 1 ) 2 ) 9 ) 0 ) 1 ) 9 ) ] ) ) 7 ) ] ) ) ( ( ( ( ( ( ( ( [MAX 0 ) 9 ) 4 ) 4 ) ( ( ( ( ( ( ( ( ( [MED 9 ) 3 ) 2 ) 1 ) 9 ) 4 ) 4 ) 5 ) ] ) ) 5 ) 6 ) ] ) ) 9 ) 3 ) 0 ) ( ( ( ( [MED 5 ) 6 ) ( ( ( ( ( ( ( [MAX 1 ) 9 ) 4 ) 1 ) 8 ) 8 ) ] ) ) ] ) ) 2 ) 5 ) ] ) ) 3 ) 0 ) 1 ) ( ( ( [MAX 0 ) 2 ) ] ) ) 8 ) ] ) ) ( ( ( ( ( ( ( ( ( [MED ( ( ( ( ( ( ( ( [MAX 2 ) 7 ) ( ( ( ( ( ( ( ( [MAX ( ( ( ( ( [SM 5 ) 7 ) 9 ) 3 ) ] ) ) ( ( ( ( ( ( ( [SM 7 ) 0 ) 6 ) 3 ) 2 ) 7 ) ] ) ) 5 ) 8 ) 2 ) 6 ) 2 ) ] ) ) 3 ) ( ( ( ( ( ( ( ( ( ( [SM ( ( ( ( ( ( ( ( [MAX 4 ) 7 ) 7 ) 7 ) 9 ) 7 ) 0 ) ] ) ) 7 ) 9 ) 4 ) 6 ) 5 ) 9 ) 2 ) 9 ) ] ) ) 5 ) 2 ) ] ) ) 8 ) 9 ) 0 ) 8 ) ( ( ( ( ( ( [MIN 3 ) 6 ) 6 ) 7 ) 3 ) ] ) ) 3 ) 6 ) ] ) ) ] ) ) ] ) ) 8 ) 6 ) ] ) ) ] ) ) 8 ) 1 ) 9 ) 7 ) ( ( ( [SM 8 ) ( ( ( ( ( ( ( ( [MIN ( ( ( ( [MAX 7 ) 9 ) 2 ) ] ) ) ( ( ( ( [MIN 1 ) 3 ) 8 ) ] ) ) 4 ) 0 ) ( ( ( ( ( ( ( ( [MIN 5 ) 6 ) ( ( ( ( ( ( ( ( ( ( [MIN 5 ) ( ( ( ( ( ( ( ( ( ( [MIN 9 ) 9 ) 4 ) ( ( ( ( [MIN 9 ) 3 ) 0 ) ] ) ) ( ( ( ( [MAX 6 ) 0 ) ( ( ( ( ( [MIN 4 ) 0 ) ( ( ( ( [MED 8 ) 3 ) 0 ) ] ) ) 8 ) ] ) ) ] ) ) 2 ) 7 ) 6 ) 6 ) ] ) ) 5 ) ( ( ( ( [MED ( ( ( ( ( ( ( ( [SM 4 ) 6 ) 8 ) 9 ) 8 ) ( ( ( ( ( [MED 3 ) ( ( ( ( ( [MIN 8 ) 1 ) 8 ) 1 ) ] ) ) 3 ) 3 ) ] ) ) 9 ) ] ) ) 1 ) 4 ) ] ) ) 2 ) 6 ) 5 ) 1 ) 3 ) ] ) ) 4 ) 8 ) ( ( ( ( ( ( ( ( ( ( [MED ( ( ( ( ( ( ( ( ( ( [MIN 6 ) 1 ) 9 ) 4 ) 9 ) 4 ) 8 ) 4 ) 6 ) ] ) ) 4 ) 5 ) 3 ) ( ( ( ( [MED 6 ) 8 ) 1 ) ] ) ) 5 ) 3 ) ( ( ( ( ( ( ( [SM 5 ) 9 ) 6 ) ( ( ( ( [MED ( ( ( ( ( ( ( ( ( ( ( [MIN 2 ) 4 ) 5 ) 9 ) 8 ) 4 ) 7 ) 3 ) 1 ) 5 ) ] ) ) 0 ) 1 ) ] ) ) 4 ) ( ( ( ( ( ( ( ( ( ( [MAX 2 ) 7 ) 5 ) 6 ) ( ( ( [MED 5 ) 7 ) ] ) ) 8 ) 8 ) 0 ) 8 ) ] ) ) ] ) ) 1 ) ] ) ) 6 ) ] ) ) 0 ) 9 ) ] ) ) ] ) ) ] )\n",
      "Expected Label: 7\n",
      "Predicted Label: 4\n"
     ]
    }
   ],
   "source": [
    "def test_random_line(file_path, model_path):\n",
    "    \"\"\"\n",
    "    Reads a file with text and labels, picks a random line, classifies the text using the model,\n",
    "    and prints the predicted and expected labels.\n",
    "\n",
    "    :param file_path: Path to the file containing text and labels.\n",
    "    :param model_path: Path to the trained model checkpoints.\n",
    "    \"\"\"\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Read the file and get a random line\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    random_line = random.choice(lines).strip()\n",
    "    text, expected_label = random_line.split('\\t')\n",
    "    \n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Expected Label: {expected_label}\")\n",
    "    print(f\"Predicted Label: {predicted_class}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"E:\\Projects\\listops-1000\\\\basic_test.tsv\"  # Replace with your file path\n",
    "model_path = \"checkpoint-4690\"  # Replace with your model checkpoints path\n",
    "test_random_line(file_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the trained model checkpoints\n",
    "model_path = \"checkpoint-4690\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def classify_text(text):\n",
    "    \"\"\"\n",
    "    Classifies the given text using a model from the specified path.\n",
    "\n",
    "    :param text: The input text to classify.\n",
    "    :param model_path: Path to the trained model checkpoints.\n",
    "    :return: Predicted label (int).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(\"[MAX 2 9 [MIN 4 7 ] 0 ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n\u001b[0;32m     10\u001b[0m     text, expected_label \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(expected_label))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mclassify_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get the predicted class\u001b[39;00m\n\u001b[0;32m     26\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:798\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    795\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    796\u001b[0m         )\n\u001b[1;32m--> 798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    544\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    545\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m         output_attentions,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:495\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    492\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    496\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:429\u001b[0m, in \u001b[0;36mFFN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:434\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    432\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    433\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m--> 434\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ilyas\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    correct = 0\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    del lines[0]\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        text, expected_label = line.strip().split('\\t')\n",
    "        predicted = classify_text(text)\n",
    "        correct += (predicted == int(expected_label))\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Line {idx + 1}\")\n",
    "\n",
    "\n",
    "    acc = correct / len(lines)\n",
    "    return acc\n",
    "\n",
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
